{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75da0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_tsplib(filename):\n",
    "    states_coords = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "    section = False\n",
    "    for line in lines:\n",
    "        if line.upper().startswith('NODE_COORD_SECTION'):\n",
    "            section = True\n",
    "            continue\n",
    "        if line.upper().startswith('EOF'):\n",
    "            break\n",
    "        if section:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                i = int(parts[0])\n",
    "                x, y = float(parts[1]), float(parts[2])\n",
    "                states_coords[i] = (x, y)\n",
    "    states_coords = [states_coords[k] for k in sorted(states_coords.keys())]\n",
    "    return states_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(states_coords): \n",
    "    n = len(states_coords)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                x1, y1 = states_coords[i]\n",
    "                x2, y2 = states_coords[j]\n",
    "                distance_matrix[i, j] = math.hypot(x1 - x2, y1 - y2)\n",
    "    return distance_matrix\n",
    "                \n",
    "def reward_matrix(distance_matrix): \n",
    "     return -distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60327f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D_df = pd.DataFrame(distance_matrix)\n",
    "#D_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R_df = pd.DataFrame(reward_matrix)\n",
    "#R_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687bf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy(Q, s, actions, epsilon):\n",
    "    \n",
    "    n_a = random.random()\n",
    "\n",
    "    if n_a < epsilon:\n",
    "        # Ação aleatória (a_a)\n",
    "        a = random.choice(actions)\n",
    "    else:\n",
    "        # Aação gulosa (a*)\n",
    "        q_vals = [Q[s, act] for act in actions]\n",
    "        best_index = int(np.argmax(q_vals))  \n",
    "        a = actions[best_index]\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(reward_matrix, alpha=0.1, gamma=0.9, epsilon=0.2, episodes=1000):\n",
    "    n = R.shape[0]\n",
    "    Q = np.zeros_like(R)\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "            # inicia em um estado aleatório\n",
    "            s = random.randint(0, n - 1)\n",
    "\n",
    "            while True:\n",
    "                \n",
    "                # Escolha a ação a (e-greedy)\n",
    "                possible_actions = [a for a in range(n) if a != s]\n",
    "                if not possible_actions:\n",
    "                    break\n",
    "                a = epsilon_greedy(Q, s, possible_actions, epsilon)\n",
    "\n",
    "                # Receba a recompensa imediata r(s,a)\n",
    "                r = reward_matrix[s, a]\n",
    "\n",
    "                # Observe o novo estado s'\n",
    "                s_next = a\n",
    "\n",
    "                # Atualize o item Q(s,a)\n",
    "                Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_next, :]) - Q[s, a])\n",
    "\n",
    "                # Atualize o estado atual\n",
    "                s = s_next\n",
    "\n",
    "                # Critério de parada\n",
    "                if random.random() < 0.05:  # evitar loop infinito\n",
    "                    break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496697cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_length(route, D):\n",
    "    total = 0\n",
    "    for i in range(len(route) - 1):\n",
    "        total += D[route[i], route[i + 1]]\n",
    "    return total\n",
    "\n",
    "def best_route(Q, start=0):\n",
    "    n = Q.shape[0]\n",
    "    route = [start]\n",
    "    current = start\n",
    "    visited = {start}\n",
    "\n",
    "    for _ in range(n - 1):\n",
    "        # ações possíveis: cidades ainda não visitadas\n",
    "        possible_actions = [a for a in range(n) if a not in visited]\n",
    "        if not possible_actions:\n",
    "            break\n",
    "\n",
    "        # escolhe a ação com maior valor Q\n",
    "        next_city = possible_actions[np.argmax([Q[current, a] for a in possible_actions])]\n",
    "        route.append(next_city)\n",
    "        visited.add(next_city)\n",
    "        current = next_city\n",
    "\n",
    "    # fecha o ciclo (volta à cidade inicial)\n",
    "    route.append(start)\n",
    "    return route\n",
    "\n",
    "route = best_route(Q, start=0)\n",
    "total_distance = calculate_length(route, D)\n",
    "\n",
    "print(\"Rota aprendida:\", route)\n",
    "print(f\"Distância total: {total_distance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0dd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time  # para medir tempo\n",
    "\n",
    "def q_learning_with_tracking(R, D, alpha=0.1, gamma=0.9, epsilon=0.2, episodes=1000):\n",
    "    n = R.shape[0]\n",
    "    Q = np.zeros_like(R)\n",
    "    \n",
    "    # listas e variáveis de monitoramento\n",
    "    distances = []         \n",
    "    best_distance = float('inf')\n",
    "    best_episode = 0\n",
    "    best_route_found = None\n",
    "    \n",
    "    # inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = random.randint(0, n - 1)\n",
    "        \n",
    "        while True:\n",
    "            actions = [a for a in range(n) if a != s]\n",
    "            if not actions:\n",
    "                break\n",
    "\n",
    "            # política ε-greedy\n",
    "            if random.random() < epsilon:\n",
    "                a = random.choice(actions)\n",
    "            else:\n",
    "                a = actions[int(np.argmax([Q[s, act] for act in actions]))]\n",
    "\n",
    "            r = R[s, a]\n",
    "            s_next = a\n",
    "\n",
    "            # atualização Q-learning\n",
    "            Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_next, :]) - Q[s, a])\n",
    "            s = s_next\n",
    "\n",
    "            if random.random() < 0.05:\n",
    "                break\n",
    "        \n",
    "        # avalia rota aprendida no episódio\n",
    "        route = best_route(Q, start=0)\n",
    "        total_distance = calculate_length(route, D)\n",
    "        distances.append(total_distance)\n",
    "\n",
    "        # verifica se é a melhor até agora\n",
    "        if total_distance < best_distance:\n",
    "            best_distance = total_distance\n",
    "            best_episode = ep + 1\n",
    "            best_route_found = route\n",
    "    \n",
    "    # fim do cronômetro\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_episode = total_time / episodes\n",
    "\n",
    "    # média das distâncias\n",
    "    avg_distance = np.mean(distances)\n",
    "\n",
    "    return {\n",
    "        \"Q\": Q,\n",
    "        \"distances\": distances,\n",
    "        \"avg_distance\": avg_distance,\n",
    "        \"best_distance\": best_distance,\n",
    "        \"best_episode\": best_episode,\n",
    "        \"best_route\": best_route_found,\n",
    "        \"total_time\": total_time,\n",
    "        \"avg_time_per_episode\": avg_time_per_episode\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843946a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, distances, avg_distance, best_distance, best_episode, best_route_found = q_learning_with_tracking(\n",
    "    R, D, alpha=0.1, gamma=0.9, epsilon=0.2, episodes=2000\n",
    ")\n",
    "\n",
    "print(f\"Média das distâncias: {avg_distance:.2f}\")\n",
    "print(f\"Melhor distância encontrada: {best_distance:.2f}\")\n",
    "print(f\"Episódio da melhor rota: {best_episode}\")\n",
    "print(\"Melhor rota aprendida:\", best_route_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63822c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Episódio')\n",
    "plt.ylabel('Distância total da rota')\n",
    "plt.title('Evolução da distância ao longo do treinamento')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18978739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\\\\wsl.localhost\\Ubuntu-22.04\\home\\elisaveloso\\aprendizado_por_reforco\\berlin52.tsp\\berlin52.tsp\"\n",
    "coords = read_tsplib(\"/home/elisaveloso/aprendizado_por_reforco/berlin52.tsp/berlin52.tsp\")\n",
    "coords_small = coords[:52]   # usa só 10 cidades para teste\n",
    "D = distance_matrix(coords_small)\n",
    "R = -D                    \n",
    "\n",
    "Q = q_learning(R, alpha=0.75, gamma=0.15, epsilon=0.01, episodes=1000)\n",
    "print(\"Q-table aprendida:\")\n",
    "print(Q.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fb4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
